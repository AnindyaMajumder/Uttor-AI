# ü§ñ Uttor-AI

> An intelligent Bengali Q&A system powered by Retrieval-Augmented Generation (RAG), specifically designed for Higher Secondary Bangla.

## ÔøΩ Table of Contents

- [‚ú® Key Features](#-key-features)
- [üöÄ Quick Start](#-quick-start)
- [üßæ Sample Queries and Outputs](#-sample-queries-and-outputs)
- [üí¨ Question & Answer regarding the project](#-a-few-qa-regarding-the-project)
- [üìö API Documentation](#-api-documentation)
- [üß™ Testing with Postman](#-testing-with-postman)
- [üèóÔ∏è Project Structure](#Ô∏è-project-structure)

## ‚ú® Key Features

- üéØ **Subject-Specific Expertise**: Focused on HSC Bangla 1st Paper content
  - üß† **Smart Processing**: Bengali text processing with **bnlp_toolkit** and semantic chunking using text-embedding-3-large
- üîç **Intelligent Retrieval**: Advanced RAG system with **Pinecone** vector database and **BAAI/bge-m3** embeddings for accurate context matching
- üåê **Multilingual Input**: Accepts questions in any language 
- üìö **Book-First Approach**: Prioritizes answers from provided pdf
- üé≠ **Teacher Persona**: Responds like a knowledgeable school teacher powered by **OpenAI GPT-4.1**
- ‚ö° **RESTful API**: RESTful API built with **FastAPI** for easy integration

## üöÄ Quick Start

### Prerequisites

- Python 3.10.11 (recommended)
- OpenAI API key
- Pinecone API key
- Tesseract OCR (for PDF text extraction)
- Postman (for API testing)

### Installation

1. **Install Tesseract OCR**
   
   **On Ubuntu/Debian:**
   ```bash
   sudo apt update
   sudo apt install tesseract-ocr
   sudo apt install libtesseract-dev
   ```
   
   **On Windows:**
   - Download Tesseract installer from: https://github.com/UB-Mannheim/tesseract/wiki
   - Install and add Tesseract to your PATH environment variable

2. **Clone the repository**
   ```bash
   git clone https://github.com/AnindyaMajumder/Uttor-AI.git
   cd Uttor-AI
   ```

3. **Create a virtual environment**
   
   **Using venv (recommended):**
   ```bash
   python -m venv .venv
   ```
   or
   ```bash
   py -3.10 -m venv .venv
   ```
   
   **Activate virtual environment:**
   ```bash
   # On Linux/Mac:
   source .venv/bin/activate
   
   # On Windows (Command Prompt):
   .venv\Scripts\activate
   
   # On Windows (PowerShell):
   .venv\Scripts\Activate.ps1
   ```

4. **Install dependencies**
   1. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

5. **Set up environment variables**
   Create a `.env` file in the root directory:
   ```env
   OPENAI_API_KEY=<your_openai_api_key>
   PINECONE_API_KEY=<your_pinecone_api_key>
   INDEX_NAME=<your_index_name>
   ```

6. **Vectordb (Pinecone) Setup**
  The script `rag/vectorstore.py` is responsible for generating embeddings from your documents and storing them in the Pinecone vector database. Make sure your database and index is set up before running the main application.
  After setting up the database and running the embedding process, you can execute `app.py` to start the API server.

1. **Run the application**
   ```bash
   uvicorn app:app --host 127.0.0.1 --port 8000
   ```

   The API will be available at `http://127.0.0.1:8000`

2. **Test on Postman**
   
   Enter with `POST` request at -> `http://127.0.0.1:8000` and
   send `JSON` request as `raw body` like 
   ```
   {
     "query": "‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶∏‡ßÅ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶ï‡¶æ‡¶ï‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?"
   }
   ```
   Another `JSON` file with short-term memory will return as resonse.

## üßæ Sample Queries and Outputs

**Query (English):**
```
Hi
```

**Response:**
```
‡¶π‡ßç‡¶Ø‡¶æ‡¶≤‡ßã! ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡ßÄ ‡¶ú‡¶æ‡¶®‡¶§‡ßá ‡¶ö‡¶æ‡¶®? ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡¶ü‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®, ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶•‡ßá‡¶ï‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡¶¨‡•§
```

**Query (Bengali):**
```
‡¶ï‡¶æ‡¶ï‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡¶§‡¶æ ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?
```

**Response:**
```
‡¶¨‡¶á ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ, '‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ' ‡¶ó‡¶≤‡ßç‡¶™‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ ‡¶§‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶Æ‡¶æ‡¶ï‡ßá "‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡¶§‡¶æ" ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡•§
```

**Query (Bangla):**
```
‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?
```

**Response:**
```
‡¶¨‡¶á ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ, ‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶õ‡¶ø‡¶≤ ‡ßß‡ß´ ‡¶¨‡¶õ‡¶∞‡•§
```

**Query (English):**
```
Who is the author of the story?
```
**Response:**
```
‡¶¨‡¶á ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ, '‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ' ‡¶ó‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶∞‡¶ö‡¶Ø‡¶º‡¶ø‡¶§‡¶æ ‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶• ‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞‡•§
```

## ü§î A few Q&A regarding the project

#### 1. What method or library did you use to extract the text, and why? Did you face any formatting challenges with the PDF content?
I used `pdf2image` to convert each PDF page into an image, followed by `pytesseract` with Bengali-trained data to perform optical character recognition. This approach was necessary because, despite the fonts being selectable, they were not mapped to Unicode code points. Preprocessing steps, such as binarisation and noise reduction‚Äîwere applied to improve the clarity of the scanned text and thereby enhance recognition accuracy

#### 2. What chunking strategy did you choose (e.g. paragraph-based, sentence-based, character limit)? Why do you think it works well for semantic retrieval?
I opted for a sentence‚Äëbased segmentation method. Firstly, the raw text was cleaned using the `BNLP toolkit` to remove artefacts and standardise punctuation. Subsequently, I split on sentence boundaries detected by the semantic splitter using OPENAI's ```text-embedding-3-large``` with a breakpoint threshold of `0.8`, rather than imposing arbitrary character limits. This yields self‚Äëcontained, semantically coherent fragments that are well suited to vector‚Äëbased similarity search. For every page, it will generate numerous embeddings based on the context. I excluded pages composed primarily of MCQs, since these often lack explanatory context and could degrade retrieval precision (work as noise for this case).

#### 3. What embedding model did you use? Why did you choose it? How does it capture the meaning of the text?
For vectorisation, I selected the ```BGE‚Äëm3``` embedding model, owing to its robust support for Bangla script and its capacity to process extended passages (up to 8000 tokens). The model generates dense numerical representations that capture the semantic relationships between words and phrases, facilitating the retrieval of passages that best match a given query.

#### 4. How are you comparing the query with your stored chunks? Why did you choose this similarity method and storage setup?
Queries and document fragments are both encoded with the ```BGE‚Äëm3``` model, after which cosine similarity is computed between the query vector and each fragment vector. All vectors are stored in a Pinecone index (in cloud) for efficient approximate nearest‚Äëneighbour search using cosine similarity. Cosine similarity measures the angle between two normalised vectors, making it insensitive to text length and well-suited to comparing semantic content.

#### 5. How do you ensure that the question and the document chunks are compared meaningfully? What would happen if the query is vague or missing context?
By using the same embedding model for queries and document fragments, the system ensures that both are represented in a common semantic space. A similarity threshold is applied if the highest cosine score falls below a predetermined cutoff; the system informs the user that it cannot locate a relevant passage and suggests rephrasing or providing additional detail.

#### 6. Do the results seem relevant? If not, what might improve them (e.g. better chunking, better embedding model, larger document)?
Overall, the retrieved passages align pretty well with user queries. Mapping the font with unicode would be great for robust text extraction. Introducing post‚ÄëOCR cleaning, such as common misrecognition corrections for specific Bangla characters to reduce OCR errors. Also, labelling a small subset of query‚Äìpassage pairs manually and fine-tuning a cross‚Äëencoder reranker, thereby improving precision on top results.

## üìö API Documentation

### Base URL
```
http://127.0.0.1:8000
```

### Endpoints

#### POST `/ask`

**Request Body:**
```json
{
  "query": "string"
}
```

**Response:**
```json
{
  "messages": [
    {
      "role": "system",
      "content": "System prompt..."
    },
    {
      "role": "user", 
      "content": "User question ..."
    },
    {
      "role": "assistant",
      "content": "AI response in Bengali ..."
    }
  ]
}
```

**Example Response**
```json
{
  "messages": [
    {
      "role": "system",
      "content": "You are a chill **school teacher** who prioritizes answering from **books and authoritative sources** before providing answers. Your responses must meet these criteria:\n- Firstly try to answer from a relevant book or text. If the book-based answer is **not available or incomplete**, you may **still attempt** to respond based on your knowledge.\n- If you're **not confident** in the answer (even after attempting outside sources), respond politely:\n  - Say: ‚Äú‡¶¶‡ßÅ‡¶É‡¶ñ‡¶ø‡¶§!! ‡¶Ü‡¶Æ‡¶ø ‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§ ‡¶®‡¶á‚Äù.\n- **Answer in Bengali**, regardless of the user's language.\n- Users may ask questions in **any language**, but your responses remain in **Bengali**."
    },
    {
      "role": "user",
      "content": "Hi!"
    },
    {
      "role": "assistant",
      "content": "‡¶π‡ßç‡¶Ø‡¶æ‡¶≤‡ßã! ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡ßÄ ‡¶ú‡¶æ‡¶®‡¶§‡ßá ‡¶ö‡¶æ‡¶®? ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡¶ü‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®, ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶•‡ßá‡¶ï‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡¶¨‡•§"
    },
    {
      "role": "user",
      "content": "‡¶ï‡¶æ‡¶ï‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡¶§‡¶æ ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?"
    },
    {
      "role": "assistant",
      "content": "‡¶¨‡¶á ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ, '‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ' ‡¶ó‡¶≤‡ßç‡¶™‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ ‡¶§‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶Æ‡¶æ‡¶ï‡ßá \"‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡¶§‡¶æ\" ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡•§"
    },
    {
      "role": "user",
      "content": "‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?"
    },
    {
      "role": "assistant",
      "content": "‡¶¨‡¶á ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ, ‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶õ‡¶ø‡¶≤ ‡ßß‡ß´ ‡¶¨‡¶õ‡¶∞‡•§"
    },
    {
      "role": "user",
      "content": "Who is the author of the story?"
    },
    {
      "role": "assistant",
      "content": "‡¶¨‡¶á ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ, '‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ' ‡¶ó‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶∞‡¶ö‡¶Ø‡¶º‡¶ø‡¶§‡¶æ ‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶• ‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞‡•§"
    },
    {
      "role": "user",
      "content": "Summarise our conversation"
    },
    {
      "role": "assistant",
      "content": "‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶ì ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶ï‡¶•‡ßã‡¶™‡¶ï‡¶•‡¶®‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡¶ø‡¶™‡ßç‡¶§‡¶∏‡¶æ‡¶∞ ‡¶®‡¶ø‡¶ö‡ßá ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ ‡¶π‡¶≤‡ßã‚Äî\n\n‡ßß. ‡¶Ü‡¶™‡¶®‡¶ø ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶ú‡¶æ‡¶®‡¶§‡ßá ‡¶ö‡ßá‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤‡ßá‡¶®, ‚Äú‡¶ï‡¶æ‡¶ï‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡¶§‡¶æ ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?‚Äù\n‡¶â‡¶§‡ßç‡¶§‡¶∞‡ßá ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶•‡ßá‡¶ï‡ßá ‡¶ú‡¶æ‡¶®‡¶æ‡¶á, ‚Äò‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ‚Äô ‡¶ó‡¶≤‡ßç‡¶™‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ ‡¶§‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶Æ‡¶æ‡¶ï‡ßá ‚Äú‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡¶§‡¶æ‚Äù ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡•§\n\n‡ß®. ‡¶è‡¶∞‡¶™‡¶∞ ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ú‡¶æ‡¶®‡¶§‡ßá ‡¶ö‡ßá‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤‡ßá‡¶®, ‚Äú‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?‚Äù\n‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶•‡ßá‡¶ï‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶á, ‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶õ‡¶ø‡¶≤ ‡ßß‡ß´ ‡¶¨‡¶õ‡¶∞‡•§\n\n‡ß©. ‡¶è‡¶∞‡¶™‡¶∞ ‡¶Ü‡¶™‡¶®‡¶ø ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø‡¶§‡ßá ‡¶ú‡¶æ‡¶®‡¶§‡ßá ‡¶ö‡ßá‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤‡ßá‡¶®, ‚ÄúWho is the author of the story?‚Äù\n‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶á ‡¶•‡ßá‡¶ï‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶á, ‚Äò‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ‚Äô ‡¶ó‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶∞‡¶ö‡¶Ø‡¶º‡¶ø‡¶§‡¶æ ‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶• ‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞‡•§\n\n‡¶è‡¶á ‡¶õ‡¶ø‡¶≤ ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡¶ø‡¶™‡ßç‡¶§ ‡¶ï‡¶•‡ßã‡¶™‡¶ï‡¶•‡¶®‡•§ ‡¶Ü‡¶Æ‡¶ø ‡¶∏‡¶¨ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¨‡¶á ‡¶•‡ßá‡¶ï‡ßá ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§"
    }
  ]
}
```

## üß™ Testing with Postman

### Quick Setup
1. Import the `UttorAI_API_testing.postman_collection` file into Postman
2. Start the server: `uvicorn app:app --host 127.0.0.1 --port 8000`
3. Send requests to test the API

### Manual Setup
- **Method**: POST
- **URL**: `http://127.0.0.1:8000/ask`
- **Headers**: `Content-Type: application/json`
- **Body**:
```json
{
  "query": "‡¶ï‡¶æ‡¶ï‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡¶§‡¶æ ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?"
}
```

### Test Cases
- Bengali queries: `"‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤?"`
- English queries: `"Who is the author of the story?"`
- Greetings: `"Hi"`


## üèóÔ∏è Project Structure

```
Uttor-AI/
‚îú‚îÄ‚îÄ app.py                 # FastAPI application entry point
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ README.md             # Project documentation
‚îú‚îÄ‚îÄ UttorAI_API_testing.postman_collection # Postman API testing
‚îú‚îÄ‚îÄ LICENSE               # Project license
‚îî‚îÄ‚îÄ rag/                  # RAG implementation
    ‚îú‚îÄ‚îÄ chain.py          # Main RAG chain logic
    ‚îú‚îÄ‚îÄ vectorstore.py    # Vector store operations
    ‚îú‚îÄ‚îÄ core/             # Core RAG components
    ‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py # Embedding models
    ‚îÇ   ‚îú‚îÄ‚îÄ model.py      # Language model setup
    ‚îÇ   ‚îú‚îÄ‚îÄ retriever.py  # Document retrieval logic
    ‚îÇ   ‚îî‚îÄ‚îÄ setup.py      # Database Index status check
    ‚îú‚îÄ‚îÄ data/             # Dataset corpus for context 
    ‚îÇ   ‚îî‚îÄ‚îÄ HSC26-Bangla1st-Paper.pdf
    ‚îî‚îÄ‚îÄ preprocessing/    # Data processing
        ‚îú‚îÄ‚îÄ clean_text.py # Text cleaning utilities
        ‚îú‚îÄ‚îÄ loader.py     # Document loaders
        ‚îî‚îÄ‚îÄ splitter.py   # Text chunking with semantic splitter
```
